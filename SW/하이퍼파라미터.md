# buffer_size
 - buffer_size=int(1e6),
 - buffer_size: 리플레이 버퍼의 크기입니다. 버퍼 크기를 늘리면 더 많은 경험을 저장할 수 있지만, 메모리 사용량이 증가합니다. 작은 버퍼 크기는 이전 경험을 더 자주 덮어쓰게 됩니다.

# learning_rate
  - learning_rate=1e-3,
  - learning_rate: 학습률입니다. 학습률이 크면 모델이 빠르게 학습하지만, 너무 크면 학습이 불안정해질 수 있습니다. 학습률이 작으면 학습이 안정적이지만 느리게 진행됩니다.

# gamma
  - gamma=0.95(99로 할 계획)
  - gamma: 감마 값 (할인율)입니다. 미래 보상의 현재 가치에 대한 가중치입니다.
  - 큰 감마 값 (예: 0.99): 장기적인 보상을 중요시합니다.
  - 작은 감마 값 (예: 0.90): 단기적인 보상을 더 중시합니다.

# batch_size
  - batch_size=256,(516으로 할 계획)
  - batch_size: 배치 크기입니다. 한 번에 몇 개의 샘플을 사용할지 결정합니다. 큰 배치 크기는 더 안정적인 학습을 제공하지만, 메모리 사용량이 증가합니다. 작은 배치 크기는 노이즈가 많은 학습을 제공할 수 있습니다.

# policy_kwargs
  - policy_kwargs=dict(net_arch=[256, 256, 256]),
  - net_arch: 정책 네트워크의 아키텍처를 정의합니다. 여기서는 세 개의 은닉층(각각 256개의 뉴런)으로 구성된 네트워크를 사용합니다. 네트워크가 깊고 넓을수록 모델의 표현력이 높아지지만, 학습이 더 오래 걸릴 수 있습니다.
  - 활성화 함수 고민

# learning_starts
  - learning_starts=10000,
  - learning_starts: 학습을 시작하기 전에 환경과 상호작용하는 스텝 수입니다. 초기 탐색 단계를 위해 설정됩니다. 이 값을 늘리면 초기 탐색을 더 많이 하지만, 학습이 늦게 시작됩니다. 이 값을 줄이면 모델이 더 빨리 학습을 시작하지만, 충분한 초기 탐색을 하지 않을 수 있습니다.


  # 환경 생성
config = {
    "observation": {
        "type": "Kinematics",
        "features": ["x", "y", "vx", "vy"]
    },
    "action": {
        "type": "ContinuousAction",
        "longitudinal": True,
        "lateral": True
    },
    "reward": {
        "type": "ParkingReward"
    },
    "scaling": 1.75,
    "controlled_vehicles": 1,
    "initial_vehicle_count": 10,
    "duration": 50,
    "screen_width": 600,
    "screen_height": 150,
    "centering_position": [0.5, 0.5],
    "collision_reward": -1,
    "lane_centering_cost": 0.05,
    "high_speed_reward": 0.4,
    "vehicles_count": 1,
    "other_vehicles_type": "highway_env.vehicle.behavior.IDMVehicle",
    "policy_frequency": 10,
    "max_speed": 10,
    "max_acceleration": 5,
    "max_steering_angle": np.radians(20),
}